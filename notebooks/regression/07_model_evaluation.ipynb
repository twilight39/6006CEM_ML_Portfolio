{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5427ab60-5282-4a8c-a99f-9b638d3899b6",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "## Goals\n",
    "1. Load all trained models (Ridge, XGBoost baseline, XGBoost tuned, Neural Network, Ensemble)\n",
    "2. Evaluate on Test Set (previously unseen data)\n",
    "3. Compare all models side-by-side\n",
    "4. Test set performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c0d64555-4ded-4680-9015-71630660eb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Effy/Documents/Projects/ML_Portfolio/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f856459-fb00-4e4b-a379-ac7fb4d25bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "CLEANED_FILE_PATH: str = \"../../data/cleaned/regression\"\n",
    "MODELS_SAVE_PATH: str = \"../../models/regression\"\n",
    "FIGURES_SAVE_PATH: str = \"../../figures\"\n",
    "\n",
    "RANDOM_STATE: int = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ccf340",
   "metadata": {},
   "source": [
    "## Load Test Set and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c063ff73-f83d-47b4-ad85-025af54df5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data and model loaded successfully\n",
      "X_train shape: (15786, 54)\n",
      "X_test shape: (3383, 54)\n"
     ]
    }
   ],
   "source": [
    "# Load Test Set\n",
    "output_dir = Path(CLEANED_FILE_PATH)\n",
    "\n",
    "X_test = pl.read_csv(output_dir / \"X_test_engineered.csv\")\n",
    "y_test_log = pl.read_csv(output_dir / \"y_test_log.csv\")[\"annual_salary_log\"]\n",
    "y_test_orig = pl.read_csv(output_dir / \"y_test.csv\")[\"annual_salary\"]\n",
    "\n",
    "print(\"Test Set Loaded:\")\n",
    "print(f\"  X_test shape: {X_test.shape}\")\n",
    "print(f\"  y_test_log shape: {y_test_log.shape}\")\n",
    "print(f\"  y_test_orig shape: {y_test_orig.shape}\")\n",
    "\n",
    "# Convert to numpy for sklearn models\n",
    "X_test_np = X_test.to_numpy()\n",
    "y_test_log_np = y_test_log.to_numpy()\n",
    "y_test_orig_np = y_test_orig.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a2533",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "# Load Models\n",
    "models_dir = Path(MODELS_SAVE_PATH)\n",
    "\n",
    "print(\"Loading trained models...\\n\")\n",
    "\n",
    "# Traditional ML Models\n",
    "ridge_model = joblib.load(models_dir / \"ridge_model.pkl\")\n",
    "print(\"✓ Ridge Regression loaded\")\n",
    "\n",
    "xgb_baseline = joblib.load(models_dir / \"xgb_model.pkl\")\n",
    "print(\"✓ XGBoost Baseline loaded\")\n",
    "\n",
    "xgb_tuned = joblib.load(models_dir / \"xgb_tuned.pkl\")\n",
    "print(\"✓ XGBoost Tuned loaded\")\n",
    "\n",
    "ensemble_model = joblib.load(models_dir / \"stacking_model.pkl\")\n",
    "print(\"✓ Stacking Ensemble loaded\")\n",
    "\n",
    "\n",
    "# First, define the architecture (must match training)\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLPRegressor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "# Load NN model\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "nn_model = MLPRegressor(X_test.shape[1]).to(device)\n",
    "nn_model.load_state_dict(torch.load(models_dir / \"mlp_model_best.pth\"))\n",
    "nn_model.eval()\n",
    "print(f\"✓ Neural Network loaded (device: {device})\")\n",
    "\n",
    "print(\"\\n✓ All models loaded successfully\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f728fca-aa7b-4564-9583-e12b32c49cd5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2a525ed6-4c4f-4eb6-b5ec-869f2c1bd727",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SHAP values for global explainability...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Effy/Documents/Projects/ML_Portfolio/.venv/lib/python3.12/site-packages/xgboost/data.py:1320: UserWarning: Unknown data type: <class 'shap.explainers._tree.TreeExplainer'>, trying to convert it to csr_matrix\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Not supported type for data.<class 'shap.explainers._tree.TreeExplainer'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Calculate SHAP values\u001b[39;00m\n\u001b[32m      6\u001b[39m explainer = shap.Explainer(best_model)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m shap_values = \u001b[43mexplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexplainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ SHAP values computed\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ML_Portfolio/.venv/lib/python3.12/site-packages/shap/explainers/_tree.py:380\u001b[39m, in \u001b[36mTreeExplainer.__call__\u001b[39m\u001b[34m(self, X, y, interactions, check_additivity, approximate)\u001b[39m\n\u001b[32m    377\u001b[39m     feature_names = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdata_feature_names\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interactions:\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m     v = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_call\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_additivity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_additivity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapproximate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapproximate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    382\u001b[39m         v = np.stack(v, axis=-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# put outputs at the end\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ML_Portfolio/.venv/lib/python3.12/site-packages/shap/explainers/_tree.py:564\u001b[39m, in \u001b[36mTreeExplainer.shap_values\u001b[39m\u001b[34m(self, X, y, tree_limit, approximate, check_additivity, from_call)\u001b[39m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, xgboost.core.DMatrix):\n\u001b[32m    562\u001b[39m     \u001b[38;5;66;03m# Retrieve any DMatrix properties if they have been set on the TreeEnsemble Class\u001b[39;00m\n\u001b[32m    563\u001b[39m     dmatrix_props = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33m_xgb_dmatrix_props\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     X = \u001b[43mxgboost\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdmatrix_props\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    565\u001b[39m phi = \u001b[38;5;28mself\u001b[39m.model.original_model.predict(\n\u001b[32m    566\u001b[39m     X,\n\u001b[32m    567\u001b[39m     iteration_range=(\u001b[32m0\u001b[39m, n_iterations),\n\u001b[32m   (...)\u001b[39m\u001b[32m    570\u001b[39m     validate_features=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    571\u001b[39m )\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_additivity \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.model_output == \u001b[33m\"\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ML_Portfolio/.venv/lib/python3.12/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ML_Portfolio/.venv/lib/python3.12/site-packages/xgboost/core.py:885\u001b[39m, in \u001b[36mDMatrix.__init__\u001b[39m\u001b[34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[39m\n\u001b[32m    882\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    883\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m885\u001b[39m handle, feature_names, feature_types = \u001b[43mdispatch_data_backend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnthread\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_split_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_split_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    895\u001b[39m \u001b[38;5;28mself\u001b[39m.handle = handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ML_Portfolio/.venv/lib/python3.12/site-packages/xgboost/data.py:1505\u001b[39m, in \u001b[36mdispatch_data_backend\u001b[39m\u001b[34m(data, missing, threads, feature_names, feature_types, enable_categorical, data_split_mode)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m converted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1497\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_scipy_csr(\n\u001b[32m   1498\u001b[39m         data=converted,\n\u001b[32m   1499\u001b[39m         missing=missing,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1502\u001b[39m         feature_types=feature_types,\n\u001b[32m   1503\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1505\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNot supported type for data.\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(data)))\n",
      "\u001b[31mTypeError\u001b[39m: Not supported type for data.<class 'shap.explainers._tree.TreeExplainer'>"
     ]
    }
   ],
   "source": [
    "# Evaluation Functions\n",
    "def inverse_transform_salary(log_salary_pred: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert log-transformed salaries back to original scale\"\"\"\n",
    "    return np.expm1(log_salary_pred)\n",
    "\n",
    "\n",
    "def evaluate_sklearn_model(\n",
    "    model, X: np.ndarray, y_true_log: np.ndarray, model_name: str\n",
    ") -> dict:\n",
    "    \"\"\"Evaluate sklearn models on test set\"\"\"\n",
    "    y_pred_log = model.predict(X)\n",
    "\n",
    "    # Log scale metrics\n",
    "    rmse_log = np.sqrt(mean_squared_error(y_true_log, y_pred_log))\n",
    "    mae_log = mean_absolute_error(y_true_log, y_pred_log)\n",
    "    r2_log = r2_score(y_true_log, y_pred_log)\n",
    "\n",
    "    # Original scale metrics\n",
    "    y_true_orig = inverse_transform_salary(y_true_log)\n",
    "    y_pred_orig = inverse_transform_salary(y_pred_log)\n",
    "\n",
    "    rmse_orig = np.sqrt(mean_squared_error(y_true_orig, y_pred_orig))\n",
    "    mae_orig = mean_absolute_error(y_true_orig, y_pred_orig)\n",
    "    r2_orig = r2_score(y_true_orig, y_pred_orig)\n",
    "\n",
    "    print(f\"--- {model_name} ---\")\n",
    "    print(f\"RMSE (log): {rmse_log:.4f}\")\n",
    "    print(f\"MAE (log):  {mae_log:.4f}\")\n",
    "    print(f\"R² (log):   {r2_log:.4f}\")\n",
    "    print(f\"RMSE ($):   ${rmse_orig:,.0f}\")\n",
    "    print(f\"MAE ($):    ${mae_orig:,.0f}\")\n",
    "    print(f\"R² ($):     {r2_orig:.4f}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"rmse_log\": rmse_log,\n",
    "        \"mae_log\": mae_log,\n",
    "        \"r2_log\": r2_log,\n",
    "        \"rmse_orig\": rmse_orig,\n",
    "        \"mae_orig\": mae_orig,\n",
    "        \"r2_orig\": r2_orig,\n",
    "        \"y_pred_orig\": y_pred_orig,\n",
    "        \"y_pred_log\": y_pred_log,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_nn_model(\n",
    "    model, X: np.ndarray, y_true_log: np.ndarray, model_name: str\n",
    ") -> dict:\n",
    "    \"\"\"Evaluate PyTorch neural network on test set\"\"\"\n",
    "    X_tensor = torch.tensor(X.astype(np.float32)).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_log = model(X_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    # Log scale metrics\n",
    "    rmse_log = np.sqrt(mean_squared_error(y_true_log, y_pred_log))\n",
    "    mae_log = mean_absolute_error(y_true_log, y_pred_log)\n",
    "    r2_log = r2_score(y_true_log, y_pred_log)\n",
    "\n",
    "    # Original scale metrics\n",
    "    y_true_orig = inverse_transform_salary(y_true_log)\n",
    "    y_pred_orig = inverse_transform_salary(y_pred_log)\n",
    "\n",
    "    rmse_orig = np.sqrt(mean_squared_error(y_true_orig, y_pred_orig))\n",
    "    mae_orig = mean_absolute_error(y_true_orig, y_pred_orig)\n",
    "    r2_orig = r2_score(y_true_orig, y_pred_orig)\n",
    "\n",
    "    print(f\"--- {model_name} ---\")\n",
    "    print(f\"RMSE (log): {rmse_log:.4f}\")\n",
    "    print(f\"MAE (log):  {mae_log:.4f}\")\n",
    "    print(f\"R² (log):   {r2_log:.4f}\")\n",
    "    print(f\"RMSE ($):   ${rmse_orig:,.0f}\")\n",
    "    print(f\"MAE ($):    ${mae_orig:,.0f}\")\n",
    "    print(f\"R² ($):     {r2_orig:.4f}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"rmse_log\": rmse_log,\n",
    "        \"mae_log\": mae_log,\n",
    "        \"r2_log\": r2_log,\n",
    "        \"rmse_orig\": rmse_orig,\n",
    "        \"mae_orig\": mae_orig,\n",
    "        \"r2_orig\": r2_orig,\n",
    "        \"y_pred_orig\": y_pred_orig,\n",
    "        \"y_pred_log\": y_pred_log,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "362ee931-a808-48a3-a445-728dd006e2dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The beeswarm plot requires an `Explanation` object as the `shap_values` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Feature names\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mshap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplots\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbeeswarm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshap_values\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/ML_Portfolio/.venv/lib/python3.12/site-packages/shap/plots/_beeswarm.py:111\u001b[39m, in \u001b[36mbeeswarm\u001b[39m\u001b[34m(shap_values, max_display, order, clustering, cluster_threshold, color, axis_color, alpha, ax, show, log_scale, color_bar, s, plot_size, color_bar_label, group_remaining_features)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(shap_values, Explanation):\n\u001b[32m    110\u001b[39m     emsg = \u001b[33m\"\u001b[39m\u001b[33mThe beeswarm plot requires an `Explanation` object as the `shap_values` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(emsg)\n\u001b[32m    113\u001b[39m sv_shape = shap_values.shape\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sv_shape) == \u001b[32m1\u001b[39m:\n",
      "\u001b[31mTypeError\u001b[39m: The beeswarm plot requires an `Explanation` object as the `shap_values` argument."
     ]
    }
   ],
   "source": [
    "# Evaluate Models on Test Set\n",
    "print(\"Evaluating all models on test set:\\n\")\n",
    "\n",
    "# Evaluate each model\n",
    "ridge_results = evaluate_sklearn_model(\n",
    "    ridge_model, X_test_np, y_test_log_np, \"Ridge Regression\"\n",
    ")\n",
    "xgb_baseline_results = evaluate_sklearn_model(\n",
    "    xgb_baseline, X_test_np, y_test_log_np, \"XGBoost Baseline\"\n",
    ")\n",
    "xgb_tuned_results = evaluate_sklearn_model(\n",
    "    xgb_tuned, X_test_np, y_test_log_np, \"XGBoost Tuned\"\n",
    ")\n",
    "ensemble_results = evaluate_sklearn_model(\n",
    "    ensemble_model, X_test_np, y_test_log_np, \"Stacking Ensemble\"\n",
    ")\n",
    "nn_results = evaluate_nn_model(nn_model, X_test_np, y_test_log_np, \"Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2227995",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "### Individual Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec326add-e654-48ed-a76f-c73f10b0d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison Table\n",
    "comparison_data = {\n",
    "    \"Model\": [\n",
    "        \"Ridge Regression\",\n",
    "        \"XGBoost Baseline\",\n",
    "        \"XGBoost Tuned\",\n",
    "        \"Stacking Ensemble\",\n",
    "        \"Neural Network\",\n",
    "    ],\n",
    "    \"RMSE (log)\": [\n",
    "        ridge_results[\"rmse_log\"],\n",
    "        xgb_baseline_results[\"rmse_log\"],\n",
    "        xgb_tuned_results[\"rmse_log\"],\n",
    "        ensemble_results[\"rmse_log\"],\n",
    "        nn_results[\"rmse_log\"],\n",
    "    ],\n",
    "    \"MAE (log)\": [\n",
    "        ridge_results[\"mae_log\"],\n",
    "        xgb_baseline_results[\"mae_log\"],\n",
    "        xgb_tuned_results[\"mae_log\"],\n",
    "        ensemble_results[\"mae_log\"],\n",
    "        nn_results[\"mae_log\"],\n",
    "    ],\n",
    "    \"R² (log)\": [\n",
    "        ridge_results[\"r2_log\"],\n",
    "        xgb_baseline_results[\"r2_log\"],\n",
    "        xgb_tuned_results[\"r2_log\"],\n",
    "        ensemble_results[\"r2_log\"],\n",
    "        nn_results[\"r2_log\"],\n",
    "    ],\n",
    "    \"RMSE ($)\": [\n",
    "        ridge_results[\"rmse_orig\"],\n",
    "        xgb_baseline_results[\"rmse_orig\"],\n",
    "        xgb_tuned_results[\"rmse_orig\"],\n",
    "        ensemble_results[\"rmse_orig\"],\n",
    "        nn_results[\"rmse_orig\"],\n",
    "    ],\n",
    "    \"MAE ($)\": [\n",
    "        ridge_results[\"mae_orig\"],\n",
    "        xgb_baseline_results[\"mae_orig\"],\n",
    "        xgb_tuned_results[\"mae_orig\"],\n",
    "        ensemble_results[\"mae_orig\"],\n",
    "        nn_results[\"mae_orig\"],\n",
    "    ],\n",
    "    \"R² ($)\": [\n",
    "        ridge_results[\"r2_orig\"],\n",
    "        xgb_baseline_results[\"r2_orig\"],\n",
    "        xgb_tuned_results[\"r2_orig\"],\n",
    "        ensemble_results[\"r2_orig\"],\n",
    "        nn_results[\"r2_orig\"],\n",
    "    ],\n",
    "}\n",
    "\n",
    "comparison_df = pl.DataFrame(comparison_data).sort(\"RMSE ($)\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be76fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model\n",
    "best_model_idx = comparison_df[\"RMSE ($)\"].arg_min()\n",
    "best_model_name = comparison_df[\"Model\"][best_model_idx]\n",
    "best_rmse = comparison_df[\"RMSE ($)\"][best_model_idx]\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Test RMSE: ${best_rmse:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca05130",
   "metadata": {},
   "source": [
    "### Individual Models vs Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626180be",
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_models = comparison_df.filter(\n",
    "    pl.col(\"Model\").is_in(\n",
    "        [\"Ridge Regression\", \"XGBoost Baseline\", \"XGBoost Tuned\", \"Neural Network\"]\n",
    "    )\n",
    ")\n",
    "ensemble_models = comparison_df.filter(pl.col(\"Model\").is_in([\"Stacking Ensemble\"]))\n",
    "\n",
    "print(\n",
    "    \"Individual Models Average RMSE:\", f\"${individual_models['RMSE ($)'].mean():,.0f}\"\n",
    ")\n",
    "print(\"Ensemble Models Average RMSE:\", f\"${ensemble_models['RMSE ($)'].mean():,.0f}\")\n",
    "\n",
    "ensemble_improvement = (\n",
    "    ensemble_models[\"RMSE ($)\"].mean() - individual_models[\"RMSE ($)\"].mean()\n",
    ")\n",
    "print(\n",
    "    f\"\\nEnsemble Improvement: ${ensemble_improvement:,.0f} ({ensemble_improvement / individual_models['RMSE ($)'].mean() * 100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582a18c",
   "metadata": {},
   "source": [
    "In our case, ensemble models seem to provide negligible benefit. This could be due to:\n",
    "1. **Low Model Diversity**: All models hit similar performance ceiling (~$50k RMSE)\n",
    "2. **Data Limitations**: Limited complementary errors to correct\n",
    "3. **Performance Ceiling**: Performance ceiling due to data limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21c3c74",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58268310",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "metrics = [\n",
    "    (\"RMSE ($)\", \"RMSE ($)\", \"#e74c3c\"),\n",
    "    (\"MAE ($)\", \"MAE ($)\", \"#3498db\"),\n",
    "    (\"R² ($)\", \"R² ($)\", \"#2ecc71\"),\n",
    "]\n",
    "\n",
    "for (metric_label, col_name, color), ax in zip(metrics, axes):\n",
    "    values = comparison_df[col_name].to_list()\n",
    "    models = comparison_df[\"Model\"].to_list()\n",
    "\n",
    "    # Color the best one differently\n",
    "    colors = [color if i != best_model_idx else \"#f39c12\" for i in range(len(models))]\n",
    "\n",
    "    bars = ax.bar(\n",
    "        models, values, color=colors, alpha=0.8, edgecolor=\"black\", linewidth=1.5\n",
    "    )\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        label = (\n",
    "            f\"${val:,.0f}\"\n",
    "            if \"MAE\" in metric_label or \"RMSE\" in metric_label\n",
    "            else f\"{val:.4f}\"\n",
    "        )\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height,\n",
    "            label,\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "    ax.set_title(metric_label, fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_ylabel(metric_label, fontsize=11)\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    Path(FIGURES_SAVE_PATH) / \"final_model_comparison.png\", dpi=300, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65da107",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_name == \"Ridge Regression\":\n",
    "    best_y_pred_orig = ridge_results[\"y_pred_orig\"]\n",
    "elif best_model_name == \"XGBoost Baseline\":\n",
    "    best_y_pred_orig = xgb_baseline_results[\"y_pred_orig\"]\n",
    "elif best_model_name == \"XGBoost Tuned\":\n",
    "    best_y_pred_orig = xgb_tuned_results[\"y_pred_orig\"]\n",
    "elif best_model_name == \"Stacking Ensemble\":\n",
    "    best_y_pred_orig = ensemble_results[\"y_pred_orig\"]\n",
    "else:  # Neural Network\n",
    "    best_y_pred_orig = nn_results[\"y_pred_orig\"]\n",
    "\n",
    "residuals = y_test_orig_np - best_y_pred_orig\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residual plot\n",
    "axes[0].scatter(\n",
    "    best_y_pred_orig, residuals, alpha=0.5, s=20, edgecolors=\"black\", linewidth=0.5\n",
    ")\n",
    "axes[0].axhline(0, color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "axes[0].set_xlabel(\"Predicted Salary ($)\", fontsize=11, fontweight=\"bold\")\n",
    "axes[0].set_ylabel(\"Residuals ($)\", fontsize=11, fontweight=\"bold\")\n",
    "axes[0].set_title(f\"Residual Plot ({best_model_name})\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Prediction vs Actual\n",
    "axes[1].scatter(\n",
    "    y_test_orig_np, best_y_pred_orig, alpha=0.5, s=20, edgecolors=\"black\", linewidth=0.5\n",
    ")\n",
    "axes[1].plot(\n",
    "    [y_test_orig_np.min(), y_test_orig_np.max()],\n",
    "    [y_test_orig_np.min(), y_test_orig_np.max()],\n",
    "    \"r--\",\n",
    "    linewidth=2,\n",
    "    label=\"Perfect Prediction\",\n",
    ")\n",
    "axes[1].set_xlabel(\"Actual Salary ($)\", fontsize=11, fontweight=\"bold\")\n",
    "axes[1].set_ylabel(\"Predicted Salary ($)\", fontsize=11, fontweight=\"bold\")\n",
    "axes[1].set_title(\n",
    "    f\"Prediction vs Actual ({best_model_name})\", fontsize=12, fontweight=\"bold\"\n",
    ")\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    Path(FIGURES_SAVE_PATH) / \"best_model_residuals.png\", dpi=300, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Residual Analysis ({best_model_name}):\")\n",
    "print(f\"  Mean Residual: ${residuals.mean():,.0f}\")\n",
    "print(f\"  Std Residual: ${residuals.std():,.0f}\")\n",
    "print(f\"  95% CI: ${1.96 * residuals.std():,.0f}\")\n",
    "print(f\"  Min Residual: ${residuals.min():,.0f}\")\n",
    "print(f\"  Max Residual: ${residuals.max():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3111c8c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d86ba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Error Distribution Analysis\n",
    "print(\"Error Distribution by Salary Range:\\n\")\n",
    "\n",
    "# Define salary bands\n",
    "salary_bands = [\n",
    "    (15000, 50000, \"Low (<$50k)\"),\n",
    "    (50000, 75000, \"Lower-Mid ($50-75k)\"),\n",
    "    (75000, 100000, \"Mid ($75-100k)\"),\n",
    "    (100000, 150000, \"Upper-Mid ($100-150k)\"),\n",
    "    (150000, 500000, \"High (>$150k)\"),\n",
    "]\n",
    "\n",
    "# Use best model predictions\n",
    "errors_by_band = []\n",
    "\n",
    "for low, high, label in salary_bands:\n",
    "    mask = (y_test_orig_np >= low) & (y_test_orig_np < high)\n",
    "    if mask.sum() > 0:\n",
    "        band_actuals = y_test_orig_np[mask]\n",
    "        band_preds = best_y_pred_orig[mask]\n",
    "        band_errors = np.abs(band_actuals - band_preds)\n",
    "\n",
    "        errors_by_band.append(\n",
    "            {\n",
    "                \"Salary Band\": label,\n",
    "                \"Count\": mask.sum(),\n",
    "                \"Mean Error\": band_errors.mean(),\n",
    "                \"Median Error\": np.median(band_errors),\n",
    "                \"RMSE\": np.sqrt(mean_squared_error(band_actuals, band_preds)),\n",
    "            }\n",
    "        )\n",
    "\n",
    "errors_df = pl.DataFrame(errors_by_band)\n",
    "display(errors_df)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.bar(\n",
    "    errors_df[\"Salary Band\"],\n",
    "    errors_df[\"Mean Error\"],\n",
    "    color=\"#e74c3c\",\n",
    "    alpha=0.7,\n",
    "    label=\"Mean Absolute Error\",\n",
    ")\n",
    "ax.plot(\n",
    "    errors_df[\"Salary Band\"],\n",
    "    errors_df[\"RMSE\"],\n",
    "    color=\"#3498db\",\n",
    "    marker=\"o\",\n",
    "    linewidth=2,\n",
    "    markersize=8,\n",
    "    label=\"RMSE\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Salary Band\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Error ($)\", fontweight=\"bold\")\n",
    "ax.set_title(\"Prediction Error by Salary Range\", fontweight=\"bold\")\n",
    "ax.legend()\n",
    "ax.tick_params(axis=\"x\", rotation=45)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(FIGURES_SAVE_PATH) / \"error_by_salary_band.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "worst_band = errors_df.sort(\"RMSE\", descending=True)[0]\n",
    "print(f\"  Highest error in: {worst_band['Salary Band'][0]}\")\n",
    "print(f\"  RMSE: ${worst_band['RMSE'][0]:,.0f}\")\n",
    "print(\n",
    "    f\"  → Model struggles most with {'high earners' if 'High' in worst_band['Salary Band'] else 'this salary range'}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681318e3",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pl.read_csv(output_dir / \"X_train_engineered.csv\")\n",
    "y_train_log = pl.read_csv(output_dir / \"Y_train_log.csv\")\n",
    "\n",
    "scoring = {\n",
    "    \"rmse\": \"neg_root_mean_squared_error\",\n",
    "    \"mae\": \"neg_mean_absolute_error\",\n",
    "    \"r2\": \"r2_score\",\n",
    "}\n",
    "\n",
    "models_for_cv = {\n",
    "    \"Ridge Regression\": ridge_model,\n",
    "    \"XGBoost Baseline\": xgb_baseline,\n",
    "    \"XGBoost Tuned\": xgb_tuned,\n",
    "    \"Stacking Ensemble\": ensemble_model,\n",
    "}\n",
    "\n",
    "cv_results_dict = {}\n",
    "\n",
    "print(\"5-Fold Cross-Validation on Training Set:\\n\")\n",
    "\n",
    "for model_name, model in models_for_cv.items():\n",
    "    # Perform 5-fold cross-validation on training set\n",
    "    cv_scores = cross_validate(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train_log,\n",
    "        cv=5,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        n_jobs=-1,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    # Convert back to positive RMSE\n",
    "    train_rmse_cv = np.sqrt(-cv_scores[\"train_score\"])\n",
    "    val_rmse_cv = np.sqrt(-cv_scores[\"test_score\"])\n",
    "\n",
    "    cv_results_dict[model_name] = {\n",
    "        \"train_rmse_mean\": train_rmse_cv.mean(),\n",
    "        \"train_rmse_std\": train_rmse_cv.std(),\n",
    "        \"val_rmse_mean\": val_rmse_cv.mean(),\n",
    "        \"val_rmse_std\": val_rmse_cv.std(),\n",
    "    }\n",
    "\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Train RMSE (CV): {train_rmse_cv.mean():.4f} ± {train_rmse_cv.std():.4f}\")\n",
    "    print(f\"  Val RMSE (CV):   {val_rmse_cv.mean():.4f} ± {val_rmse_cv.std():.4f}\")\n",
    "    print(f\"  Overfitting Gap: {(train_rmse_cv.mean() - val_rmse_cv.mean()):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa15c61",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f345eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_salary = np.median(y_test_orig_np)\n",
    "mean_salary = np.mean(y_test_orig_np)\n",
    "salary_range = np.max(y_test_orig_np) - np.min(y_test_orig_np)\n",
    "\n",
    "print(\"Test Set Salary Statistics:\")\n",
    "print(f\"  Median: ${median_salary:,.0f}\")\n",
    "print(f\"  Mean: ${mean_salary:,.0f}\")\n",
    "print(f\"  Range: ${salary_range:,.0f}\")\n",
    "\n",
    "best_rmse_orig = comparison_df[\"RMSE ($)\"][best_model_idx]\n",
    "best_mae_orig = comparison_df[\"MAE ($)\"][best_model_idx]\n",
    "\n",
    "print(f\"\\n{best_model_name} Performance:\")\n",
    "print(\n",
    "    f\"  RMSE: ${best_rmse_orig:,.0f} ({best_rmse_orig / median_salary * 100:.1f}% of median)\"\n",
    ")\n",
    "print(\n",
    "    f\"  MAE: ${best_mae_orig:,.0f} ({best_mae_orig / median_salary * 100:.1f}% of median)\"\n",
    ")\n",
    "print(f\"  Average prediction is off by ~${best_mae_orig:,.0f}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
